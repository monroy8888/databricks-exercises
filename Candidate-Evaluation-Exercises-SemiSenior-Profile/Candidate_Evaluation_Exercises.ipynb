{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8c94a6-2fe4-41ac-8a40-663fd9d230a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Candidate Evaluation Exercises for SemiSenior Profile\n",
    "\n",
    "This notebook contains exercises designed to evaluate a candidate's proficiency in Python programming, PySpark data processing, and AWS Cloud data solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfa21cc-d11d-4d6d-957b-4cd2f74b5680",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Python Programming Exercise\n",
    "\n",
    "**Task:** Write a Python class that represents a simple bank account. The class should have methods to deposit, withdraw, and check the balance, with basic error handling for withdrawal limits.\n",
    "\n",
    "```python\n",
    "class BankAccount:\n",
    "    def __init__(self, initial_balance=0):\n",
    "        self.balance = initial_balance\n",
    "\n",
    "    def deposit(self, amount):\n",
    "        self.balance += amount\n",
    "        return self.balance\n",
    "\n",
    "    def withdraw(self, amount):\n",
    "        if amount > self.balance:\n",
    "            raise ValueError(\"Insufficient funds\")\n",
    "        self.balance -= amount\n",
    "        return self.balance\n",
    "\n",
    "    def get_balance(self):\n",
    "        return self.balance\n",
    "\n",
    "# Example usage\n",
    "account = BankAccount(100)\n",
    "account.deposit(50)\n",
    "print(account.get_balance())  # Output: 150\n",
    "try:\n",
    "    account.withdraw(200)\n",
    "except ValueError as e:\n",
    "    print(e)  # Output: Insufficient funds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c140c6c-2bae-42ab-895d-cdaf80d79bee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5822921-b30b-491d-bcae-668c0060a80d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25636784-4633-4e88-bec5-98376136dc94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## PySpark Data Processing Exercise\n",
    "\n",
    "**Task:** Given a PySpark DataFrame `df` with columns `name` and `salary`, write a PySpark query to calculate the average salary and filter out individuals earning more than the average salary.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getMaster(\"local\").getOrCreate()\n",
    "data = [(\"Alice\", 50000), (\"Bob\", 40000), (\"Charlie\", 70000)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"salary\"])\n",
    "\n",
    "average_salary = df.agg(avg(col(\"salary\")).alias(\"average_salary\"))\n",
    "df = df.join(average_salary)\n",
    "df_filtered = df.filter(col(\"salary\") > col(\"average_salary\"))\n",
    "df_filtered.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd9df9f-ebf1-4dc4-870d-b2ea4833a63e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca72937-9e1c-4612-9a48-1ba9f0005e09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212890de-2a85-4d3b-b759-0f1fad6022f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## AWS Cloud Data Solutions Exercise\n",
    "\n",
    "**Task:** Design a cloud-based data pipeline using AWS services that ingests, processes, and visualizes large datasets. The solution should ensure data security, be cost-effective, and scale based on demand.\n",
    "\n",
    "### Detailed Requirements\n",
    "1. **Data Ingestion:** Automate the ingestion of large, structured datasets into the cloud using AWS Glue.\n",
    "2. **Data Storage:** Use Amazon S3 for raw data storage, employing partitioning to improve performance. Processed data can be stored in Amazon Redshift for analysis.\n",
    "3. **Data Processing:** Utilize AWS Glue for ETL jobs and consider Amazon Kinesis for real-time data processing needs.\n",
    "4. **Data Visualization:** Implement Amazon QuickSight for dashboard creation and data visualization.\n",
    "5. **Security and Compliance:** Secure data using AWS KMS, IAM roles, and ensure all data transfers are encrypted using HTTPS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65bbd5f9-80bf-4496-b10e-3a069743e81a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faffd4a2-c440-438a-8ca2-fd99d0fcc6f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Candidate_Evaluation_Exercises",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
